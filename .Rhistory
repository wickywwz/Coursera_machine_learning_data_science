#write.csv(rank_job_relation_disxy1,"综合相关度三列（r=top10均值）.csv")
相似度<-as.vector(further_job_distxy)
scatter.smooth(相似度)
hist(相似度,breaks = 100,freq = FALSE)
lines(density(相似度))
abline(v= mean(相似度))
mean(相似度)
相似度<-as.vector(further_job_distxy)
scatter.smooth(相似度)
相似度<-as.vector(further_job_distx)
scatter.smooth(相似度)
相似度<-as.vector(further_job_disty)
scatter.smooth(相似度)
further_job_distxy<-(further_job_distx+further_job_disty)/2
rank_job_relation_disxy1<-as.data.frame(as.table(further_job_distxy))
rank_job_relation_disxy1<-arrange(rank_job_relation_disxy1, desc(Freq))
rank_job_relation_disxy<-as.data.frame(further_job_distxy)
相似度<-as.vector(further_job_distxy)
scatter.smooth(相似度)
further_job<-table(NewData_job2$zhineng,NewData_job2$next_job)
further_job<-further_job[-1,-1]
further_job1<-further_job
k<-1
for(k in 1:85){further_job
further_job<-table(NewData_job2$zhineng,NewData_job2$next_job)
further_job<-further_job[-1,-1]
further_job1<-further_job
k<-1
for(k in 1:85){further_job1[k,k]<-max(further_job[-k,k])}
further_job2<-further_job
further_job_distx <- further_job2
further_job_disty <- further_job2
i<-1
j<-1
for(i in 1:85){
for(j in 1:85){
further_job_distx1[i,j]<-further_job_distx[i,j]/sum(further_job_distx[,j])-sum(further_job_distx[i,])/sum(further_job_distx)
further_job_disty1[i,j]<-further_job_disty[i,j]/sum(further_job_disty[i,])-sum(further_job_disty[,j])/sum(further_job_disty)
}
}
further_job_distx <- further_job_distx1
further_job_disty <- further_job_disty1
#svdx<-svd(t(further_job_distx))
#svdy<-svd(further_job_disty)
#further_job_distx <- further_job_distx[1:70,]
#further_job_disty <- further_job_disty[,1:70]
#further_job_distx[1:70,] <- t(svdx$u[,1:70])
#further_job_disty[,1:70] <- svdy$u[,1:70]
further_job_distx<-scale(further_job_distx[,1:85])
further_job_disty<-t(scale(t(further_job_disty[,1:85])))
further_job_distx<-as.matrix(dist(t(further_job_distx)))
further_job_disty<-as.matrix(dist(further_job_disty))
further_job_distx1<-further_job_distx
further_job_disty1<-further_job_disty
diag(further_job_distx1)<-max(further_job_distx1)
diag(further_job_disty1)<-max(further_job_disty1)
u<-1
v<-1
for(u in 1:85){
for(v in 1:85){
if(u==v){
further_job_distx[u,v]<-1
further_job_disty[u,v]<-1
}
else{
further_job_distx[u,v]<-(max(further_job_distx1)-further_job_distx[u,v])/(max(further_job_distx1)-min(further_job_distx1))
further_job_disty[u,v]<-(max(further_job_disty1)-further_job_disty[u,v])/(max(further_job_disty1)-min(further_job_disty1))
}
}
}
further_job_distxy<-(further_job_distx+further_job_disty)/2
#further_job_distxy<-sqrt(further_job_distxy)
rank_job_relation_disx1<-as.data.frame(as.table(further_job_distx))
rank_job_relation_disx1<-arrange(rank_job_relation_disx1, desc(Freq))
rank_job_relation_disx<-as.data.frame(further_job_distx)
#write.csv(rank_job_relation_disx,"目标工作相关度（top10均值）.csv")
#write.csv(rank_job_relation_disx1,"目标工作相关度三列（top10均值）.csv")
rank_job_relation_disy1<-as.data.frame(as.table(further_job_disty))
rank_job_relation_disy1<-arrange(rank_job_relation_disy1, desc(Freq))
rank_job_relation_disy<-as.data.frame(further_job_disty)
#write.csv(rank_job_relation_disy,"前置工作相关度（top10均值）.csv")
#write.csv(rank_job_relation_disy1,"前置工作相关度三列（top10均值）.csv")
rank_job_relation_disxy1<-as.data.frame(as.table(further_job_distxy))
rank_job_relation_disxy1<-arrange(rank_job_relation_disxy1, desc(Freq))
rank_job_relation_disxy<-as.data.frame(further_job_distxy)
#write.csv(rank_job_relation_disxy,"综合相关度（r=top10均值）.csv")
#write.csv(rank_job_relation_disxy1,"综合相关度三列（r=top10均值）.csv")
相似度<-as.vector(further_job_distxy)
scatter.smooth(相似度)
相似度<-as.vector(further_job_distx)
scatter.smooth(相似度)
further_job<-table(NewData_job2$zhineng,NewData_job2$next_job)
further_job<-further_job[-1,-1]
further_job1<-further_job
k<-1
for(k in 1:85){further_job1[k,k]<-max(further_job[-k,k])}
further_job2<-further_job
l<-1
r<-0.025
n<-10
for(l in 1:85){
top<-as.vector(further_job2[l,])
topN<-top[order(top,decreasing = TRUE)][1:n]
#topN<-top[top > sum(top)*r]
further_job2[l,l]<-mean(topN)
}
further_job_distx <- further_job2
further_job_disty <- further_job2
i<-1
j<-1
for(i in 1:85){
for(j in 1:85){
further_job_distx1[i,j]<-further_job_distx[i,j]/sum(further_job_distx[,j])-sum(further_job_distx[i,])/sum(further_job_distx)
further_job_disty1[i,j]<-further_job_disty[i,j]/sum(further_job_disty[i,])-sum(further_job_disty[,j])/sum(further_job_disty)
}
}
further_job_distx <- further_job_distx1
further_job_disty <- further_job_disty1
#svdx<-svd(t(further_job_distx))
#svdy<-svd(further_job_disty)
#further_job_distx <- further_job_distx[1:70,]
#further_job_disty <- further_job_disty[,1:70]
#further_job_distx[1:70,] <- t(svdx$u[,1:70])
#further_job_disty[,1:70] <- svdy$u[,1:70]
further_job_distx<-scale(further_job_distx[,1:85])
further_job_disty<-t(scale(t(further_job_disty[,1:85])))
further_job_distx<-as.matrix(dist(t(further_job_distx)))
further_job_disty<-as.matrix(dist(further_job_disty))
further_job_distx1<-further_job_distx
further_job_disty1<-further_job_disty
diag(further_job_distx1)<-max(further_job_distx1)
diag(further_job_disty1)<-max(further_job_disty1)
u<-1
v<-1
for(u in 1:85){
for(v in 1:85){
if(u==v){
further_job_distx[u,v]<-1
further_job_disty[u,v]<-1
}
else{
further_job_distx[u,v]<-(max(further_job_distx1)-further_job_distx[u,v])/(max(further_job_distx1)-min(further_job_distx1))
further_job_disty[u,v]<-(max(further_job_disty1)-further_job_disty[u,v])/(max(further_job_disty1)-min(further_job_disty1))
}
}
}
further_job_distxy<-(further_job_distx+further_job_disty)/2
相似度<-as.vector(further_job_distx)
scatter.smooth(相似度)
相似度<-as.vector(further_job_disty)
scatter.smooth(相似度)
相似度<-as.vector(further_job_distxy)
scatter.smooth(相似度)
rank_job_relation_disx1<-as.data.frame(as.table(further_job_distx))
rank_job_relation_disx1<-arrange(rank_job_relation_disx1, desc(Freq))
rank_job_relation_disx<-as.data.frame(further_job_distx)
#write.csv(rank_job_relation_disx,"目标工作相关度（top10均值）.csv")
#write.csv(rank_job_relation_disx1,"目标工作相关度三列（top10均值）.csv")
rank_job_relation_disy1<-as.data.frame(as.table(further_job_disty))
rank_job_relation_disy1<-arrange(rank_job_relation_disy1, desc(Freq))
rank_job_relation_disy<-as.data.frame(further_job_disty)
#write.csv(rank_job_relation_disy,"前置工作相关度（top10均值）.csv")
#write.csv(rank_job_relation_disy1,"前置工作相关度三列（top10均值）.csv")
rank_job_relation_disxy1<-as.data.frame(as.table(further_job_distxy))
rank_job_relation_disxy1<-arrange(rank_job_relation_disxy1, desc(Freq))
rank_job_relation_disxy<-as.data.frame(further_job_distxy)
#write.csv(rank_job_relation_disxy,"综合相关度（r=top10均值）.csv")
#write.csv(rank_job_relation_disxy1,"综合相关度三列（r=top10均值）.csv")
further_job_distxy<-sqrt(further_job_distxy)
相似度<-as.vector(further_job_distxy)
scatter.smooth(相似度)
rank_job_relation_disxy1<-as.data.frame(as.table(further_job_distxy))
rank_job_relation_disxy1<-arrange(rank_job_relation_disxy1, desc(Freq))
rank_job_relation_disxy<-as.data.frame(further_job_distxy)
write.csv(rank_job_relation_disxy,"综合相关度（r=top10均值）1027.csv")
write.csv(rank_job_relation_disxy1,"综合相关度三列（r=top10均值）1027.csv")
hist(相似度,breaks = 100,freq = FALSE)
lines(density(相似度))
abline(v= mean(相似度))
mean(相似度)
library(data.table)
library(dplyr)
a<-matrix(c(1:25))
a
a<-matrix(c(1:25),5)
a
svd(a)
sum(a[,1]^2)
svd<-svd(a)
sum(svd$u[,1]^2)
sum(svd$u[,2]^2)
sum(svd$u[,3]^2)
sum(svd$u[,4]^2)
a<-matrix(rnorm(250),50)
svd<-svd(a)
sum(svd$u[,4]^2)
sum(svd$u[,1]^2)
x=1
x
x <- 1
x
plot(cars)
# Create motivating plot
x <- runif(1000, 0, 40)
x
y <- matrix(1:20, nrow=5, ncol=4, byrow=TRUE)
y
mymatrix <- matrix(cells, nrows=2, ncol=2, byrow=True, dimnames=list(rnames,cnames))
cells <- c(1,26,24,68)
rnames <- c('R1','R2')
cnames <- c('C1','C2')
mymatrix <- matrix(cells, nrows=2, ncol=2, byrow=True, dimnames=list(rnames,cnames))
mymatrix <- matrix(cells, nrow=2, ncol=2, byrow=True, dimnames=list(rnames,cnames))
cells <- c(1,26,24,68)
rnames <- c('R1','R2')
cnames <- c('C1','C2')
mymatrix <- matrix(cells, nrow=2, ncol=2, byrow=True, dimnames=list(rnames,cnames))
cells <- c(1,26,24,68)
rnames <- c('R1','R2')
cnames <- c('C1','C2')
mymatrix <- matrix(cells, nrow=2, ncol=2, byrow=TRUE, dimnames=list(rnames,cnames))
y <- matrix(1:20, nrow=5, ncol=4, byrow=TRUE) #创建5*4矩阵，数值1-20，按行填充
cells <- c(1,26,24,68)
rnames <- c('R1','R2')
cnames <- c('C1','C2')
mymatrix <- matrix(cells, nrow=2, ncol=2, byrow=TRUE, dimnames=list(rnames,cnames))
y <- matrix(1:20, nrow=5, ncol=4, byrow=TRUE) #创建5*4矩阵，数值1-20，按行填充
cells <- c(1,26,24,68)
rnames <- c('R1','R2')
cnames <- c('C1','C2')
mymatrix <- matrix(cells, nrow=2, ncol=2, byrow=TRUE, dimnames=list(rnames,cnames))
mymatrix
dim1 <- c('A1','A2')
dim2 <- c('B1','B2','B3')
dim3 <- c('C1','C2','C3','C4')
z <- array(1:24,c(2,3,4),dimnames=list(dim1,dim2,dim3))
z
patientID <- c(1,2,3,4)
age <- c(25,34,28,52)
diabetes <- c('Type1','Type2','Type1','Type2')
status <- c('Poor','Improved','Excellent','Poor')
patientdata <- data.frame(patientID,age,diabetes,status)
patientdata[1:2]
dose <- c(20,30,40,45,60)
drugA <- c(16,20,27,40,60)
drugB <- c(15,18,25,31,40)
help(plot)
plot(dose,drugA,type='b') #绘制简单折线图
library('UsingR')
install.packages('UsingR')
data(galton)
library('UsingR')
data(galton)
library(reshape)
library(reshape2)
long <- melt?
long <- melt(galton)
long <- melt(galton)
View(long)
g <- ggplot(long, aes(x=value,fill=variable))
g <- g+geom_histogram(colour='black',binwidth = 1)
g <- g+facet_grid(.~variable)
g
library(manipulate)
install.packages(manipulate)
install.packages("manipulate")
library(manipulate)
myHist <- function(mu){
mse <- mean((galton$child - mu)^2)
g <- ggplot(galton, aes(x = child)) + geom_histogram(fill = "salmon", colour\
= "black", binwidth=1)
g <- g + geom_vline(xintercept = mu, size = 3)
g <- g + ggtitle(paste("mu = ", mu, ", MSE = ", round(mse, 2), sep = ""))
g
}
myHist <- function(mu){
mse <- mean((galton$child - mu)^2)
g <- ggplot(galton, aes(x = child)) + geom_histogram(fill = "salmon", colour= "black", binwidth=1)
g <- g + geom_vline(xintercept = mu, size = 3)
g <- g + ggtitle(paste("mu = ", mu, ", MSE = ", round(mse, 2), sep = ""))
g
}
manipulate(myHist(mu), mu = slider(62, 74, step = 0.5))
y <- galton$child
x <- galton$parent
beta1 <- cor(y,x)*sd(y)/sd(x)
beta1
beta0 <- mean(y) - beta1*mean(x)
rbind(c(beta0, beta1), coef(lm(y ~ x)))
library(UsingR)
data(diamond)
library(ggplot2)
g = ggplot(diamond, aes(x = carat, y = price))
g = g + xlab("Mass (carats)")
g = g + ylab("Price (SIN $)")
g = g + geom_point(size = 7, colour = "black", alpha=0.5)
g = g + geom_point(size = 5, colour = "blue", alpha=0.2)
g = g + geom_smooth(method = "lm", colour = "black")
g
fit <- lm(price ~ carat, data = diamond)
coef(fit)
fit2 <- lm(price ~ I(carat - mean(carat)), data = diamond)
coef(fit2)
fit3 <- lm(price ~ I(carat * 10), data = diamond)
coef(fit3)
newx <- c(0.16, 0.27, 0.34)
predict(fit,data=newx)
data.frame(carat=newx)
predict(fit,newdata = data.frame(carat=newx))
resid(fit)
summary(fit)$sigma
pt?
a
?pt
require(datasets)
data(swiss)
swiss
View(swiss)
summary(lm(Fertility ~ . , data = swiss))
summary(lm(Fertility ~ Agriculture, data = swiss))$coefficients
z <- swiss$Agriculture + swiss$Education
lm(formula = Fertility ~ . + z, data = swiss)
data(InsectSprays)
require(stats)
g = ggplot(data = InsectSprays, aes(y = count, x = spray, fill = spray))
g = g + geom_violin(colour = "black", size = 2)
g = g + xlab("Type of spray") + ylab("Insect count")
g
summary(lm(count ~ spray - 1, data = InsectSprays))$coef
swiss = mutate(swiss, CatholicBin = 1 * (Catholic > 50))
library(dplyr)
swiss = mutate(swiss, CatholicBin = 1 * (Catholic > 50))
swiss
g = ggplot(swiss, aes(x = Agriculture, y = Fertility, colour = factor(CatholicBi\
n)))
g = ggplot(swiss, aes(x = Agriculture, y = Fertility, colour = factor(CatholicBin)))
g = g + geom_point(size = 6, colour = "black") + geom_point(size = 4)
g = g + xlab("% in Agriculture") + ylab("Fertility")
g
feelings <-c("sad","afraid")
for (i in feelings)
print(switch(i,
afraid="there is nothing to fear",
angry ="calm down now",
sad = "cheer up"))
getwd()
setwd('D:/coursera/DataScience/machine_learning')
na.strings
read.csv()
?read.csv
library('carot')
library('caret')
train_df = read.csv("pml-training.csv", na.strings=c("", "NA"))
test_df = read.csv("pml-testing.csv", na.strings=c("", "NA"))
View(swiss)
View(test_df)
View(train_df)
train_df[8:10]
length(train_df)
train_df <- train_df[8:length(train_df)]
remove_col <- colSums(is.na(train_df))
train_df <- train_df[,remove_col==0]
library('caret')
train_df <- read.csv("pml-training.csv", na.strings=c("", "NA"))
test_set <- read.csv("pml-testing.csv", na.strings=c("", "NA"))
train_df <- train_df[8:length(train_df)]
remove_col <- colSums(is.na(train_df))
train_df <- train_df[,remove_col==0]
train_len <- createDataPartition(train_df$clsse,p=0.75)[[1]]
train_len <- createDataPartition(train_df$classe,p=0.75)[[1]]
train_set <- train_df[train_len,]
valid_set <- train_df[-train_len,]
correlMatrix <- cor(train_set[, -length(train_set)])
corrplot(correlMatrix, type = "lower", tl.cex = 0.8)
library('caret')
library(corrplot)
install.packages('corrplot')
install.packages('randomForest')
library('caret')
library('corrplot')
library('randomForest')
set.seed(12306)
correlMatrix <- cor(train_set[, -length(train_set)])
corrplot(correlMatrix, type = "lower", tl.cex = 0.8)
rf_model <- randomForest(classe~., data=train_set, preprocessing="pca")
print(rf_model)
val_result = predict(rf_model,valid_set)
print ("RF Model Cross Validataion");
confusionMatrix(valid_set$classe, val_result)
install.packages('e1071')
library('e1071)
val_result = predict(rf_model,valid_set)
print ("RF Model Cross Validataion");
confusionMatrix(valid_set$classe, val_result)
accuracy = confusionMatrix(valid_set$classe, val_result)$overall['Accuracy']
library('e1071')
val_result = predict(rf_model,valid_set)
print ("RF Model Cross Validataion");
confusionMatrix(valid_set$classe, val_result)
accuracy = confusionMatrix(valid_set$classe, val_result)$overall['Accuracy']
error = (1 - accuracy) * 100
print("Out of sample error: ")
print(round(error, digits = 2))
library('caret')
library('corrplot')
library('randomForest')
library('e1071')
set.seed(12306)
val_result = predict(rf_model,valid_set)
print ("RF Model Cross Validataion");
confusionMatrix(valid_set$classe, val_result)
accuracy = confusionMatrix(valid_set$classe, val_result)$overall['Accuracy']
error = (1 - accuracy) * 100
print("Out of sample error: ")
print(round(error, digits = 2))
val_result = predict(rf_model,valid_set)
print ("RF Model Cross Validataion");
confusionMatrix(valid_set$classe, val_result)
accuracy = confusionMatrix(valid_set$classe, val_result)$overall['Accuracy']
error = (1 - accuracy) * 100
print("Out of sample error: ");print(round(error, digits = 2))
val_result = predict(rf_model,valid_set)
print ("RF Model Cross Validataion");
confusionMatrix(valid_set$classe, val_result)
accuracy = confusionMatrix(valid_set$classe, val_result)$overall['Accuracy']
error = (1 - accuracy) * 100
print("Out of sample error:",round(error, digits = 2))
#Apply same processing procedure as training set
test_df <- test_df[8:length(test_df)]
test_df <- test_df[,remove_col==0]
#Fit our model to testing data
test_result <- predict(rf_model,test_df)
print(test_df)
#Apply same processing procedure as training set
test_df <- test_df[8:length(test_df)]
test_df <- test_df[,remove_col==0]
library('caret')
library('corrplot')
library('randomForest')
library('e1071')
set.seed(12306)
library('caret')
library(corrplot)
library(randomForest)
set.seed(33355)
train_df <- read.csv("pml-training.csv", na.strings=c("", "NA"))
test_set <- read.csv("pml-testing.csv", na.strings=c("", "NA"))
train_df <- train_df[8:length(train_df)]
remove_col <- colSums(is.na(train_df))
train_df <- train_df[,remove_col==0]
train_len <- createDataPartition(train_df$classe,p=0.75)[[1]]
train_set <- train_df[train_len,]
valid_set <- train_df[-train_len,]
correlMatrix <- cor(train_set[, -length(train_set)])
corrplot(correlMatrix, type = "lower", tl.cex = 0.8)
rf_model <- randomForest(classe~., data=train_set, preprocessing="pca")
print(rf_model)
correlMatrix <- cor(train_set[, -length(train_set)])
corrplot(correlMatrix, type = "lower", tl.cex = 0.8)
rf_model <- randomForest(classe~., data=train_set, preprocessing="pca")
print(rf_model)
val_result <- predict(rf_model,valid_set)
confusionMatrix(valid_set$classe, val_result)
?randomForest
#Apply same processing procedure as training set
test_set <- read.csv("pml-testing.csv", na.strings=c("", "NA"))
test_set <- test_set[8:length(test_set)]
test_set <- test_set[,remove_col==0]
#Fit our model to testing data
test_result <- predict(rf_model,test_df)
#Apply same processing procedure as training set
test_set <- read.csv("pml-testing.csv", na.strings=c("", "NA"))
test_set <- test_set[8:length(test_set)]
test_set <- test_set[,remove_col==0]
#Fit our model to testing data
test_result <- predict(rf_model,test_set)
print(test_result)
#Apply same processing procedure as training set
test_set <- read.csv("pml-testing.csv", na.strings=c("", "NA"))
test_set <- test_set[8:length(test_set)]
test_set <- test_set[,remove_col==0]
#Fit our model to testing data
test_result <- predict(rf_model,test_set)
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
pml_write_files(answers)
#Apply same processing procedure as training set
test_set <- read.csv("pml-testing.csv", na.strings=c("", "NA"))
test_set <- test_set[8:length(test_set)]
test_set <- test_set[,remove_col==0]
#Fit our model to testing data
test_result <- predict(rf_model,test_set)
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
pml_write_files(test_result)
#Apply same processing procedure as training set
test_set <- read.csv("pml-testing.csv", na.strings=c("", "NA"))
test_set <- test_set[8:length(test_set)]
test_set <- test_set[,remove_col==0]
#Fit our model to testing data
test_result <- predict(rf_model,test_set)
print(test_result)
